{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    ALLOWED_FILE_EXTENSIONS = set([\".pdf\", \".md\", \".txt\"])\n",
    "    SEED = 42\n",
    "\n",
    "class Model:\n",
    "    NAME = \"deepseek-r1:14b\"\n",
    "    TEMPERATURE = 0.6\n",
    "\n",
    "class Preprocessing:\n",
    "    CHUNK_SIZE = 2048\n",
    "    CHUNK_OVERLAP = 128\n",
    "    EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "    RERANKER = \"ms-marco-MiniLM-L-12-v2\"\n",
    "    LLM = \"llama3.2\"\n",
    "    CONTEXTUALIZE_CHUNKS = True\n",
    "    N_SEMANTIC_RESULTS = 5\n",
    "    N_BM25_RESULTS = 5\n",
    "\n",
    "class Chatbot:\n",
    "    N_CONTEXT_RESULTS = 3\n",
    "\n",
    "class Path:\n",
    "    APP_HOME = Path(os.getenv(\"APP_HOME\", Path(__file__).parent.parent))\n",
    "    DATA_DIR = APP_HOME / \"data\"\n",
    "\n",
    "\n",
    "def configure_logging():\n",
    "    config = {\n",
    "        \"handlers\": [\n",
    "            {\n",
    "                \"sink\": sys.stdout, \n",
    "                \"colorize\": True,\n",
    "                \"format\": \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}\",\n",
    "            },\n",
    "            {\n",
    "                \"sink\": \"app.log\",  # Log to a file as well\n",
    "                \"rotation\": \"10 MB\",  # Rotate log file when it reaches 10MB\n",
    "                \"compression\": \"zip\", # Compress old log files\n",
    "                \"format\": \"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    "            },\n",
    "        ],\n",
    "        \"levels\": [\n",
    "            {\"name\": \"TRACE\", \"color\": \"<cyan>\"},\n",
    "            {\"name\": \"DEBUG\", \"color\": \"<blue>\"},\n",
    "            {\"name\": \"INFO\", \"color\": \"<green>\"},\n",
    "            {\"name\": \"WARNING\", \"color\": \"<yellow>\"},\n",
    "            {\"name\": \"ERROR\", \"color\": \"<red>\"},\n",
    "            {\"name\": \"CRITICAL\", \"color\": \"<BOLD><RED>\"},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    logger.configure(**config)\n",
    "\n",
    "# Example usage:\n",
    "configure_logging()\n",
    "\n",
    "logger.trace(\"This is a trace message.\")\n",
    "logger.debug(\"This is a debug message.\")\n",
    "logger.info(\"This is an info message.\")\n",
    "logger.warning(\"This is a warning message.\")\n",
    "logger.error(\"This is an error message.\")\n",
    "logger.critical(\"This is a critical message.\")\n",
    "\n",
    "try:\n",
    "    result = 10 / 0\n",
    "except ZeroDivisionError as e:\n",
    "    logger.exception(\"An exception occurred: {}\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Covariance: 191.20706528260865\n",
      "Unbiased Covariance: 191.20706528260865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Xs = np.array([0.0339, 0.0423, 0.213, 0.257, 0.273, 0.273, 0.450, 0.503, 0.503, \\\n",
    "0.637, 0.805, 0.904, 0.904, 0.910, 0.910, 1.02, 1.11, 1.11, 1.41, \\\n",
    "1.72, 2.03, 2.02, 2.02, 2.02])\n",
    "\n",
    "Ys = np.array([-19.3, 30.4, 38.7, 5.52, -33.1, -77.3, 398.0, 406.0, 436.0, 320.0, 373.0, \\\n",
    "93.9, 210.0, 423.0, 594.0, 829.0, 718.0, 561.0, 608.0, 1.04E3, 1.10E3, \\\n",
    "840.0, 801.0, 519.0])\n",
    "\n",
    "N = 24\n",
    "\n",
    "# Calculate the covariance (biased - dividing by N)\n",
    "covariance_biased = np.cov(Xs, Ys)[0, 1]  # [0, 1] gets the covariance from the matrix\n",
    "\n",
    "# Calculate the covariance (unbiased - dividing by N-1)\n",
    "covariance_unbiased = np.cov(Xs, Ys, ddof=1)[0, 1] # ddof=1 for unbiased\n",
    "\n",
    "print(\"Biased Covariance:\", covariance_biased)\n",
    "print(\"Unbiased Covariance:\", covariance_unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdfium2 import PdfDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(data: bytes) -> str:\n",
    "    pdf = PdfDocument(data)\n",
    "\n",
    "    content = \"\"\n",
    "    for page in pdf:\n",
    "        text_page = page.get_textpage()\n",
    "        content += f\"{text_page.get_text_bounded()}\\n\"\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"This is the first line of the paragraph.\n",
    "This is the second line of the paragraph.\n",
    "This is the third line of the paragraph.\n",
    "This is the fourth line of the paragraph.\n",
    "This is the fifth line of the paragraph.\n",
    "This is the sixth line of the paragraph.\n",
    "This is the seventh line of the paragraph.\n",
    "This is the eighth line of the paragraph.\n",
    "This is the ninth line of the paragraph.\n",
    "This is the tenth line of the paragraph.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. APJ Abdul Kalam: The Missile Man of India\n",
      "Introduction\n",
      "Dr. Avul Pakir Jainulabdeen Abdul Kalam, popularly known as the \"Missile Man of India,\" was\n",
      "a renowned scientist, visionary, and the 11th President of India. He played a crucial role in\n",
      "India's space and missile development programs and was widely admired for his simplicity,\n",
      "humility, and dedication to education and innovation. His life and works continue to inspire\n",
      "millions around the world.\n",
      "Early Life and Education\n",
      "Born on October 15, 1931, in Rameswaram, Tamil Nadu, Dr. Kalam hailed from a modest\n",
      "family. Despite financial constraints, he pursued his education with determination. He\n",
      "completed his degree in aerospace engineering from the Madras Institute of Technology (MIT).\n",
      "His passion for science and technology led him to join the Defence Research and\n",
      "Development Organisation (DRDO) and later the Indian Space Research Organisation (ISRO).\n",
      "Contributions to Science and Technology\n",
      "Dr. Kalam made significant contributions to India's missile development, earning him the title\n",
      "\"Missile Man of India.\" He played a vital role in developing the Agni and Prithvi missiles. He\n",
      "also led India's nuclear tests in 1998, making India a nuclear power. His work in aeronautics\n",
      "and defense technology placed India among the world's leading nations in space and missile\n",
      "technology.\n",
      "Presidency and Vision for India\n",
      "In 2002, Dr. Kalam was elected as the 11th President of India. During his tenure (2002-2007),\n",
      "he was known as the \"People's President\" because of his accessibility and engagement with\n",
      "young minds. He envisioned a developed India by 2020 and actively encouraged scientific\n",
      "research, innovation, and education. His book *India 2020: A Vision for the New Millennium*\n",
      "outlines his aspirations for India's growth.\n",
      "Legacy and Inspirational Thoughts\n",
      "Dr. Kalam was a strong advocate for education, urging students to dream big and work hard.\n",
      "He often said, \"Dream is not what you see in sleep; it is something that does not let you sleep.\"\n",
      "He passed away on July 27, 2015, while delivering a lecture at the Indian Institute of\n",
      "Management (IIM), Shillong. His legacy continues through his books, speeches, and the Abdul\n",
      "Kalam Foundation, which promotes education and innovation.\n",
      "Conclusion\n",
      "Dr. APJ Abdul Kalam's life is a testament to the power of dreams, perseverance, and\n",
      "dedication. His contributions to science, education, and national development make him an\n",
      "eternal source of inspiration. His vision for India and his values continue to guide future\n",
      "generations toward progress and excellence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a PDF file and pass its content as bytes to the extract_pdf_content function\n",
    "pdf_path = \"APJ_Abdul_Kalam.pdf\"\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "\tpdf_data = f.read()\n",
    "\n",
    "content = extract_pdf_content(pdf_data)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamlit.runtime.uploaded_file_manager import UploadedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.config import Config\n",
    "PDF_EXTENSION = \".pdf\"\n",
    "@dataclass\n",
    "class File:\n",
    "    name: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uploaded_file(uploaded_file: UploadedFile) -> File:\n",
    "    file_extension = Path(uploaded_file.name).suffix\n",
    "\n",
    "    if file_extension not in Config.ALLOWED_FILE_EXTENSIONS:\n",
    "        raise ValueError(f\"Invalid file extension: {file_extension} for file {uploaded_file.name}\")\n",
    "\n",
    "    if file_extension == PDF_EXTENSION:\n",
    "        return File(name=uploaded_file.name, content=extract_pdf_content(uploaded_file.getvalue()))\n",
    "\n",
    "    return File(name=uploaded_file.name, content=uploaded_file.getvalue().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File(name='APJ_Abdul_Kalam.pdf', content='Dr. APJ Abdul Kalam: The Missile Man of India\\r\\nIntroduction\\r\\nDr. Avul Pakir Jainulabdeen Abdul Kalam, popularly known as the \"Missile Man of India,\" was\\r\\na renowned scientist, visionary, and the 11th President of India. He played a crucial role in\\r\\nIndia\\'s space and missile development programs and was widely admired for his simplicity,\\r\\nhumility, and dedication to education and innovation. His life and works continue to inspire\\r\\nmillions around the world.\\r\\nEarly Life and Education\\r\\nBorn on October 15, 1931, in Rameswaram, Tamil Nadu, Dr. Kalam hailed from a modest\\r\\nfamily. Despite financial constraints, he pursued his education with determination. He\\r\\ncompleted his degree in aerospace engineering from the Madras Institute of Technology (MIT).\\r\\nHis passion for science and technology led him to join the Defence Research and\\r\\nDevelopment Organisation (DRDO) and later the Indian Space Research Organisation (ISRO).\\r\\nContributions to Science and Technology\\r\\nDr. Kalam made significant contributions to India\\'s missile development, earning him the title\\r\\n\"Missile Man of India.\" He played a vital role in developing the Agni and Prithvi missiles. He\\r\\nalso led India\\'s nuclear tests in 1998, making India a nuclear power. His work in aeronautics\\r\\nand defense technology placed India among the world\\'s leading nations in space and missile\\r\\ntechnology.\\r\\nPresidency and Vision for India\\r\\nIn 2002, Dr. Kalam was elected as the 11th President of India. During his tenure (2002-2007),\\r\\nhe was known as the \"People\\'s President\" because of his accessibility and engagement with\\r\\nyoung minds. He envisioned a developed India by 2020 and actively encouraged scientific\\r\\nresearch, innovation, and education. His book *India 2020: A Vision for the New Millennium*\\r\\noutlines his aspirations for India\\'s growth.\\r\\nLegacy and Inspirational Thoughts\\r\\nDr. Kalam was a strong advocate for education, urging students to dream big and work hard.\\r\\nHe often said, \"Dream is not what you see in sleep; it is something that does not let you sleep.\"\\nHe passed away on July 27, 2015, while delivering a lecture at the Indian Institute of\\r\\nManagement (IIM), Shillong. His legacy continues through his books, speeches, and the Abdul\\r\\nKalam Foundation, which promotes education and innovation.\\r\\nConclusion\\r\\nDr. APJ Abdul Kalam\\'s life is a testament to the power of dreams, perseverance, and\\r\\ndedication. His contributions to science, education, and national development make him an\\r\\neternal source of inspiration. His vision for India and his values continue to guide future\\r\\ngenerations toward progress and excellence.\\n')\n"
     ]
    }
   ],
   "source": [
    "from streamlit.runtime.uploaded_file_manager import UploadedFile\n",
    "\n",
    "class MockUploadedFile:\n",
    "\tdef __init__(self, name, data):\n",
    "\t\tself.name = name\n",
    "\t\tself.data = data\n",
    "\n",
    "\tdef getvalue(self):\n",
    "\t\treturn self.data\n",
    "\n",
    "# Create a mock UploadedFile object\n",
    "mock_uploaded_file = MockUploadedFile(pdf_path, pdf_data)\n",
    "\n",
    "# Load the uploaded file\n",
    "file = load_uploaded_file(mock_uploaded_file)\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    ALLOWED_FILE_EXTENSIONS = set([\".pdf\", \".md\", \".txt\"])\n",
    "    SEED = 42\n",
    "\n",
    "    class Model:\n",
    "        NAME = \"deepseek-r1:14b\"\n",
    "        TEMPERATURE = 0.6\n",
    "\n",
    "    class Preprocessing:\n",
    "        CHUNK_SIZE = 2048\n",
    "        CHUNK_OVERLAP = 128\n",
    "        EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "        RERANKER = \"ms-marco-MiniLM-L-12-v2\"\n",
    "        LLM = \"llama3.2\"\n",
    "        CONTEXTUALIZE_CHUNKS = True\n",
    "        N_SEMANTIC_RESULTS = 5\n",
    "        N_BM25_RESULTS = 5\n",
    "\n",
    "    class Chatbot:\n",
    "        N_CONTEXT_RESULTS = 3\n",
    "\n",
    "    class Path:\n",
    "        APP_HOME = Path(os.getenv(\"APP_HOME\", Path(__file__).parent.parent))\n",
    "        DATA_DIR = APP_HOME / \"data\"\n",
    "\n",
    "\n",
    "def configure_logging():\n",
    "    config = {\n",
    "        \"handlers\": [\n",
    "            {\n",
    "                \"sink\": sys.stdout, \n",
    "                \"colorize\": True,\n",
    "                \"format\": \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}\",\n",
    "            },\n",
    "            {\n",
    "                \"sink\": \"app.log\",  # Log to a file as well\n",
    "                \"rotation\": \"10 MB\",  # Rotate log file when it reaches 10MB\n",
    "                \"compression\": \"zip\", # Compress old log files\n",
    "                \"format\": \"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    "            },\n",
    "        ],\n",
    "        \"levels\": [\n",
    "            {\"name\": \"TRACE\", \"color\": \"<cyan>\"},\n",
    "            {\"name\": \"DEBUG\", \"color\": \"<blue>\"},\n",
    "            {\"name\": \"INFO\", \"color\": \"<green>\"},\n",
    "            {\"name\": \"WARNING\", \"color\": \"<yellow>\"},\n",
    "            {\"name\": \"ERROR\", \"color\": \"<red>\"},\n",
    "            {\"name\": \"CRITICAL\", \"color\": \"<bold><red>\"},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    logger.configure(**config)\n",
    "\n",
    "# Example usage:\n",
    "configure_logging()\n",
    "\n",
    "logger.trace(\"This is a trace message.\")\n",
    "logger.debug(\"This is a debug message.\")\n",
    "logger.info(\"This is an info message.\")\n",
    "logger.warning(\"This is a warning message.\")\n",
    "logger.error(\"This is an error message.\")\n",
    "logger.critical(\"This is a critical message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.exception import CustomException\n",
    "import sys\n",
    "from pypdfium2 import PdfDocument\n",
    "from streamlit.runtime.uploaded_file_manager import UploadedFile\n",
    "\n",
    "from src.config import Config\n",
    "\n",
    "TEXT_FILE_EXTENSION = \".txt\"\n",
    "MD_FILE_EXTENSION = \".md\"\n",
    "\n",
    "PDF_EXTENSION = \".pdf\"\n",
    "\n",
    "@dataclass\n",
    "class File:\n",
    "    name: str\n",
    "    content: str\n",
    "\n",
    "def extract_pdf_content(data: bytes) -> str:\n",
    "    try:\n",
    "        pdf = PdfDocument(data)\n",
    "\n",
    "        content = \"\"\n",
    "        for page in pdf:\n",
    "            text_page = page.get_textpage()\n",
    "            content += f\"{text_page.get_text_bounded()}\\n\"\n",
    "\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "\n",
    "def load_uploaded_file(uploaded_file: UploadedFile) -> File:\n",
    "    try:\n",
    "        file_extension = Path(uploaded_file.name).suffix\n",
    "\n",
    "        if file_extension not in Config.ALLOWED_FILE_EXTENSIONS:\n",
    "            raise ValueError(f\"Invalid file extension: {file_extension} for file {uploaded_file.name}\")\n",
    "\n",
    "        if file_extension == PDF_EXTENSION:\n",
    "            return File(name=uploaded_file.name, content=extract_pdf_content(uploaded_file.getvalue()))\n",
    "\n",
    "        return File(name=uploaded_file.name, content=uploaded_file.getvalue().decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from src.exception import CustomException\n",
    "import sys\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever, EnsembleRetriever\n",
    "\n",
    "from langchain_community.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from src.config import Config\n",
    "from src.file_loader import File\n",
    "\n",
    "CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You're an expert in document analysis. Your task is to provide brief, relevant context for a chunk of text.\n",
    "\n",
    "    Here is the document:\n",
    "    <document>\n",
    "\n",
    "    {document}\n",
    "    </document>\n",
    "\n",
    "    Here is the chunk we want to situate within the whole document:\n",
    "    <chunk>\n",
    "    {chunk}\n",
    "    </chunk>\n",
    "    Provide a concise context (2-3 sentences) for this chunk , considering the following guidelines:\n",
    "    1.Identify the main topic or concept discussed in the chunk.\n",
    "    2. Mention any relevant information or comparision from the broader document.\n",
    "    3.If applicable, note how this information relates to overall theme or purpose of the documents.\n",
    "    4.Include any key figure,dates,or percentages that provide importent context.\n",
    "\n",
    "    context:\n",
    "\n",
    "    \"\"\".strip()\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=Config.Preprocessing.CHUNK_SIZE,\n",
    "    chunk_overlap=Config.Preprocessing.CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "def create_llm() -> ChatOllama:\n",
    "    return ChatOllama(model=Config.Preprocessing.LLM, temperature=0, keep_alive=-1)\n",
    "\n",
    "def create_embeddings() -> FastEmbedEmbeddings:\n",
    "    return FastEmbedEmbeddings(model_name=Config.Preprocessing.EMBEDDING_MODEL)\n",
    "\n",
    "def create_reranker() -> FlashrankRerank:\n",
    "    try:\n",
    "        return FlashrankRerank(\n",
    "            model=Config.Preprocessing.RERANKER, \n",
    "            top_n=Config.Chatbot.N_CONTEXT_RESULTS\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "\n",
    "def _generate_context(llm: ChatOllama, document: str, chunk: str) -> str:\n",
    "    try:\n",
    "        messages = CONTEXT_PROMPT.format_messages(document=document, chunk=chunk)\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "\n",
    "def _create_chunks(document: Document) -> List[Document]:\n",
    "    try:\n",
    "        chunks = text_splitter.split_documents([document])\n",
    "\n",
    "        if not Config.Preprocessing.CONTEXTUALIZE_CHUNKS:\n",
    "            return chunks\n",
    "\n",
    "        llm = create_llm()\n",
    "        contextual_chunks = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            context = _generate_context(llm, document.page_content, chunk.page_content)\n",
    "\n",
    "            chunk_with_context = f\"{context}\\n\\n{chunk.page_content}\"\n",
    "            contextual_chunks.append(Document(page_content=chunk_with_context, metadata=chunk.metadata))\n",
    "\n",
    "        return contextual_chunks\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)\n",
    "\n",
    "def ingest_files(files: List[File]) -> BaseRetriever:\n",
    "    try:\n",
    "        documents = [Document(page_content=file.content, metadata={\"source\": file.name}) for file in files]\n",
    "\n",
    "        chunks = []\n",
    "        for document in documents:\n",
    "            chunks.extend(_create_chunks(document))\n",
    "\n",
    "        semantic_retriever = InMemoryVectorStore.from_documents(\n",
    "            chunks, create_embeddings()\n",
    "        ).as_retriever(search_kwargs={\"k\": Config.Preprocessing.N_SEMANTIC_RESULTS})\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        bm25_retriever.k = Config.Preprocessing.N_BM25_RESULTS  # Assign k here\n",
    "\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[semantic_retriever, bm25_retriever],\n",
    "            weights=[0.6, 0.4],\n",
    "        )\n",
    "\n",
    "        return ContextualCompressionRetriever(\n",
    "            base_compressor=create_reranker(), base_retriever=ensemble_retriever\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from src.exception import CustomException\n",
    "from src.config import Config\n",
    "from src.data_ingestion import ingest_files\n",
    "\n",
    "from src.file_loader import File\n",
    "from typing import List, TypedDict, Iterable\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import BaseMessage, Document, AIMessage, HumanMessage\n",
    "# from langchain.llms import ChatOllama\n",
    "from langchain_ollama import ChatOllama\n",
    "import sys\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You're having a conversation with an user about excerpts of their files. Try to be helpful and answer their questions.\n",
    "\n",
    "If you don't know the answer, say that you don't know and try to ask clarifying questions.\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "Here's the information you have about the excerpts of the files:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "One file can have multiple excerpts.\n",
    "\n",
    "Please, respond to the query below:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FILE_TEMPLATE=\"\"\"\n",
    "<file>\n",
    "<name>{name}</name>\n",
    "<context>{context}</context>\n",
    "</file>\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPT_TEMPLATE=ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SYSTEM_PROMPT,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\",PROMPT)\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "class Role(Enum):\n",
    "    USER=\"user\"\n",
    "    ASSISTANT=\"assistant\"\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: Role \n",
    "    content: str\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class ChunkEvent:\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class SourcesEvent:\n",
    "    content: List[Document]\n",
    "\n",
    "@dataclass\n",
    "class FinalAnswerEvent:\n",
    "    content: str\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    chat_history: List[BaseMessage]\n",
    "    context: List[Document]\n",
    "    answer:str\n",
    "    \n",
    "def _remove_thinking_from_message(message: str) -> str:\n",
    "    close_tag = \"</think>\"\n",
    "    tag_length = len(close_tag)\n",
    "    return message[message.find(close_tag) + tag_length :].strip()\n",
    "\n",
    "def create_history(welcome_message: Message) -> List[Message]:\n",
    "    return [welcome_message]\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, files: List[File]):\n",
    "        self.files = files\n",
    "        self.retriever = ingest_files(files)\n",
    "        self.llm = ChatOllama(\n",
    "            model=Config.Model.NAME,\n",
    "            temperature=Config.Model.TEMPERATURE,\n",
    "            verbose=False,\n",
    "            keep_alive=-1,\n",
    "        )\n",
    "        self.workflow = self._create_workflow()\n",
    "\n",
    "    def _format_docs(self, docs: List[Document]) -> str:\n",
    "        try:\n",
    "            return \"\\n\\n\".join(\n",
    "                FILE_TEMPLATE.format(name=doc.metadata[\"source\"], content=doc.page_content)\n",
    "                for doc in docs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "\n",
    "        \n",
    "\n",
    "    def _retrieve(self, state: State):\n",
    "        try:\n",
    "            context = self.retriever.invoke(state[\"question\"])\n",
    "            return {\"context\": context}\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "    \n",
    "    def _generate(self, state: State):\n",
    "        try:\n",
    "            messages = PROMPT_TEMPLATE.invoke(\n",
    "                {\n",
    "                    \"question\": state[\"question\"],\n",
    "                    \"context\": self._format_docs(state[\"context\"]),\n",
    "                    \"chat_history\": state[\"chat_history\"],\n",
    "                }\n",
    "            )\n",
    "            answer = self.llm.invoke(messages)\n",
    "            return {\"answer\": answer}\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "\n",
    "    def _create_workflow(self) -> CompiledStateGraph:\n",
    "        graph_builder = StateGraph(State).add_sequence([self._retrieve, self._generate])\n",
    "        graph_builder.add_edge(START, \"_retrieve\")\n",
    "        return graph_builder.compile()\n",
    "    \n",
    "    def _ask_model(\n",
    "        self, prompt: str, chat_history: List[Message]\n",
    "    ) -> Iterable[SourcesEvent | ChunkEvent | FinalAnswerEvent]:\n",
    "        try:\n",
    "            history = [\n",
    "                AIMessage(m.content) if m.role == Role.ASSISTANT else HumanMessage(m.content)\n",
    "                for m in chat_history\n",
    "            ]\n",
    "            payload = {\"question\": prompt, \"chat_history\": history}\n",
    "\n",
    "            config = {\n",
    "                \"configurable\": {\"thread_id\": 42},\n",
    "            }\n",
    "            for event_type, event_data in self.workflow.stream(\n",
    "                payload,\n",
    "                config=config,\n",
    "                stream_mode=[\"updates\", \"messages\"],\n",
    "            ):\n",
    "                if event_type == \"messages\":\n",
    "                    chunk, _ = event_data\n",
    "                    yield ChunkEvent(chunk.content)\n",
    "\n",
    "                if event_type == \"updates\":\n",
    "                    if \"_retrieve\" in event_data:\n",
    "                        documents = event_data[\"_retrieve\"][\"context\"]\n",
    "                        yield SourcesEvent(documents)\n",
    "\n",
    "                    if \"_generate\" in event_data:\n",
    "                        answer = event_data[\"_generate\"][\"answer\"]\n",
    "                        yield FinalAnswerEvent(answer.content)\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "                \n",
    "\n",
    "    def ask(\n",
    "        self, prompt: str, chat_history: List[Message]\n",
    "    ) -> Iterable[SourcesEvent | ChunkEvent | FinalAnswerEvent]:\n",
    "        try:\n",
    "            for event in self._ask_model(prompt, chat_history):\n",
    "                yield event\n",
    "                if isinstance(event, FinalAnswerEvent):\n",
    "                    response = _remove_thinking_from_message(\"\".join(event.content))\n",
    "                    chat_history.append(Message(role=Role.USER, content=prompt))\n",
    "                    chat_history.append(Message(role=Role.ASSISTANT, content=response))\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
